---
title: "computational_musicology"
author: "Daria Koppes"
date: "8-2-2021"
output:
  flexdashboard::flex_dashboard:
    storyboard: true
    theme: cosmo
---

```{r, echo=FALSE}
library(tidyverse)
library(spotifyr)
library(plotly)
library(compmus)
library(grid)
library(gridExtra)
```
```{r, echo=FALSE}
eighteen <- get_playlist_audio_features("", "37i9dQZF1EjuRTGXCnqj3q") 
nineteen <- get_playlist_audio_features("", "37i9dQZF1Ethn3c8yyjsG9") 
twenty <- get_playlist_audio_features("", "37i9dQZF1EM1eb9t3OIKaB")
billie <- get_playlist_audio_features("", "7L8fVkNZIHQ38k0hkcJkVo")
```

```{r, echo=FALSE}
combined <-
  bind_rows(
    eighteen %>% mutate(category = "Wrapped playlist 2018"),
    nineteen %>% mutate(category = "Wrapped playlist 2019"),
    twenty %>% mutate(category = "Wrapped playlist 2020")
  )

billie <-
  bind_rows(
        nineteen %>% mutate(category = "Wrapped playlist 2019"),
    billie %>% mutate(category = "Billie Eilish")
  )

```

### What about the **tempo** of the songs, does it differ over the years? 

```{r}
hist_tempo <- combined %>%
  ggplot(aes(x = tempo)) +
  geom_histogram(binwidth = 10, col="grey", fill = "#0088cc") +
    geom_density() +
  facet_wrap(~category) +
  scale_colour_brewer(        # Use the Color Brewer to choose a palette.
    type = "qual",            # Qualitative set.
    palette = "Paired"        # Name of the palette is 'Paired'.
  ) +
  theme_light() +
  theme( 
  axis.text = element_text(size = rel(0.95)), 
  strip.background = element_rect(fill="#008ae6", size=1.5, linetype="solid"),
  strip.text = element_text(size = 12),
    plot.title = element_text(size=12, 
    margin = margin(10, 0, 10, 0), hjust = 0.5)
) +
    labs(                      
    x = "Tempo",
    y = "Number of songs",
    colour = element_blank()
  ) +
    ggtitle("The tempo of the songs") 

density_tempo <- ggplot(combined, aes(x=tempo, fill = category)) +
  geom_density(alpha = 0.4) +
    scale_colour_brewer(        # Use the Color Brewer to choose a palette.
    type = "qual",            # Qualitative set.
    palette = "Paired"        # Name of the palette is 'Paired'.
  ) +
  theme_light() +
      labs(                      
    x = "Tempo",
    y = "Density",
    fill = element_blank()
  ) +
  theme(
    plot.title = element_text(size=12, 
    margin = margin(10, 0, 10, 0), hjust = 0.5)
  )+
    ggtitle("The density of songs filtered by tempo over the years") 

grid.arrange(hist_tempo, density_tempo, nrow=2)
```

### **Introduction** of the chosen corpus

PLEASE DO NOT SHOW THIS PORTFOLIO IN CLASS

The corpus consists of three of my own wrapped Spotify playlists, from the years 2018, 2019 and 2020. I chose this corpus because I want to see how the type of music I listened too has evolved over the last three years. Another research point is to find out if it is possible looking at the kind of music I was listening to, to decide the general mood of that year for me. 2019 for example was a nice year for me with a lot of traveling and fun things planned, 2020 on the other hand was a more harder year like for most people. Is this reflected in the music of my wrapped spotify playlists? 

Comparison points would be artists and features of the songs like valence and energy. I expect some artists to come back each year because I have been listening to them for a while now, e.g Imagine Dragons. Also when I travel I tend to be listening to more foreign music, so I expect 2018 and 2019 to have more Spanish/Portuguese/French artists in them than 2020 for example. I am unsure if my type of music has significantly changed over the past three years but I am curious to look into it. 

The strength of the corpus is that it contains a significant amount of representative songs and especially for 2019 and 2020 I recognize all the numbers in it. In 2019 I was also pretty obsessed with Billie Eilish so it is typical to see that 20 of the 100 songs in my wrapped playlist are by her. For 2018 however I notice that some songs in the wrapped playlist I have never listened to before. I do not know how they got in my wrapped playlist. For example: Pastempomat sang by Dawid Pdsiadlo is a polish song and I am certain that I never listened to this song or any other polish song before. I will keep these songs in the playlist to have the same number of songs in every playlist, but remember that there are some songs in the 2018 wrapped playlist that are not representable for me. 


### Lets start by comparing the **energy** of the songs

```{r, fig.align='center', fig.width=8}
histogram <- combined %>%
  ggplot(aes(x = energy)) +
  geom_histogram(binwidth = 0.1, col="black", fill = "#0088cc") +
  facet_wrap(~category) +
  scale_colour_brewer(        # Use the Color Brewer to choose a palette.
    type = "qual",            # Qualitative set.
    palette = "Paired"        # Name of the palette is 'Paired'.
  ) +
  theme_light() +
  scale_x_continuous(         # Fine-tune the x axis.
    limits = c(0, 1),
    breaks = c(0, 0.2, 0.4, 0.6, 0.8, 1),
    minor_breaks = NULL       # Remove 'minor' grid-lines.
  ) +
  theme( 
  axis.text = element_text(size = rel(0.95)), 
  strip.background = element_rect(fill="#008ae6", size=1.5, linetype="solid"),
  strip.text = element_text(size = 12),
  plot.title = element_text(size=12, 
    margin = margin(10, 0, 10, 0), hjust = 0.5)
) +
    labs(                      
    x = "Energy",
    y = "Number of songs",
    colour = element_blank()
  ) +
    ggtitle("The energy of songs over the years") 


density <- ggplot(combined, aes(x=energy, fill = category)) +
  geom_density(alpha = 0.4) +
    scale_colour_brewer(        # Use the Color Brewer to choose a palette.
    type = "qual",            # Qualitative set.
    palette = "Paired"        # Name of the palette is 'Paired'.
  ) +
  theme_light() +
  scale_x_continuous(         # Fine-tune the x axis.
    limits = c(0, 1),
    breaks = c(0, 0.2, 0.4, 0.6, 0.8, 1),
    minor_breaks = NULL       # Remove 'minor' grid-lines.
  ) +
      labs(                      
    x = "Energy",
    y = "Density",
    fill = element_blank()
  ) +
  theme(
    plot.title = element_text(size=12, 
    margin = margin(10, 0, 10, 0), hjust = 0.5)
  )+
    ggtitle("The density of songs filtered by energy over the years") 

grid.arrange(histogram, density, nrow=2)
```


***

To start looking at the differences in music throughout the year and focussing on the mood of the songs I started with a histogram comparing the energy of the songs over the three years. To my surprise the year 2019 is actually the year with the lowest amount of high energy songs. As I said in the introduction 2019 was a good year for me and i was wondering if that would show in the type of music. Maybe because it was a good year I did not need high energy music to cheer me up but could listen to more low-energy songs and that could explain the density being lower in 2019 for high energy songs in comparison to 2020 and 2018. 


### How much is my music of 2018, 2019 and 2020 alike in **mood**? 

```{r, fig.align='center', fig.width=9}
scatterplotyears <- combined %>%
  mutate(
    mode = ifelse(mode == 0, "Minor", "Major")
  ) %>%
  ggplot(                     # Set up the plot.
    aes(
      x = valence,
      y = energy,
      colour = mode
    )
  ) +
  geom_point(size = 2) +              # Scatter plot.   
  
  facet_grid(.~category) +     # Separate charts per playlist.
  scale_x_continuous(         # Fine-tune the x axis.
    limits = c(0, 1),
    breaks = c(0, 0.5, 1),   # Use grid-lines for quadrants only.
    minor_breaks = NULL       # Remove 'minor' grid-lines.
  ) +
  scale_y_continuous(         # Fine-tune the y axis in the same way.
    limits = c(0, 1),
    breaks = c(0, 0.5, 1),
    minor_breaks = NULL
  ) +
  scale_colour_brewer(        # Use the Color Brewer to choose a palette.
    type = "qual",            # Qualitative set.
    palette = "Paired"        # Name of the palette is 'Paired'.
  ) +
  
  theme_light() +             # Use a simpler theme.
  labs(                       # Make the titles nice.
    x = "Valence",
    y = "Energy",
    colour = element_blank()
  ) +
  theme( 
  axis.text = element_text(size = rel(0.95)), 
  strip.background = element_rect(fill="#008ae6", size=1.5, linetype="solid"),
  strip.text = element_text(size = 12),
  plot.title = element_text(size=12, 
    margin = margin(10, 0, 10, 0), hjust = 0.5), 
  legend.key = element_rect(fill = "#e7e7e4"),
) + 
  annotate("text", x = 0.1, y = 0.05, 
           label = "Sad" , color="black", alpha = 0.7, fontface="bold") + 
  annotate("text", x = 0.1, y = 0.95, 
           label = "Angry" , color="black", alpha = 0.7, fontface="bold") +
  annotate("text", x = 0.9, y = 0.05, 
           label = "Calm" , color="black",alpha = 0.7, fontface="bold") + 
  annotate("text", x = 0.9, y = 0.95, 
           label = "Happy" , color="black", alpha = 0.7, fontface="bold") +
  ggtitle("The mood of the songs in the different wrapped playlists") +
    geom_hline(yintercept=0.5, linetype="dashed", color = "orange") +
  geom_vline(xintercept = 0.5, linetype="dashed", 
                color = "orange")

scatterplotyears
```


***

An interesting find already is that there are not much calm songs in my Wrapped playlists, although I even have a separate playlist called 'calm' where I listen to all the time. Maybe my perception of calm songs is different than that of spotify. 
Just a reminder the dark blue minor dots mean the song sounds more dark or gloomy and the light blue major dots mean the song sounds more bright or happy. Also interesting to see is that in the sad category of low energy and low valence a lot of songs have a major mode which should make them sound more bright or happy. This is a contradictory find. 


### In depth visualisation of the influence of **Billie Eilish**

#### Comparing the mood of Billie Eilish with the mood of playlist wrapped 2019

```{r, fig.width=9, warning=FALSE}
scatterplotbillie <- billie %>%
  mutate(
    mode = ifelse(mode == 0, "Minor", "Major")
  ) %>%
  ggplot(                     # Set up the plot.
    aes(
      x = valence,
      y = energy,
      label = track.name
    )
  ) +
  geom_point(size = 1.5, color = '#008ae6', alpha = 0.6) +              # Scatter plot.   
  
  facet_wrap(~category) +     # Separate charts per playlist.
  scale_x_continuous(         # Fine-tune the x axis.
    limits = c(0, 1),
    breaks = c(0, 0.5, 1),   # Use grid-lines for quadrants only.
    minor_breaks = NULL       # Remove 'minor' grid-lines.
  ) +
  scale_y_continuous(         # Fine-tune the y axis in the same way.
    limits = c(0, 1),
    breaks = c(0, 0.5, 1),
    minor_breaks = NULL
  ) +
  scale_colour_brewer(        # Use the Color Brewer to choose a palette.
    type = "qual",            # Qualitative set.
    palette = "Paired"        # Name of the palette is 'Paired'.
  ) +
  labs(
    x = "Valence",
    y = "Energy",
    color = element_blank()
  ) +
  
  theme_light() +             # Use a simpler theme.
  
  theme( 
  strip.background = element_rect(fill="#008ae6", size=1.5, linetype="solid"), 
  legend.key = element_rect(fill = "#e7e7e4")
) + 
  annotate("text", x = 0.1, y = 0.05, 
           label = "Sad" , color="black", alpha = 0.7, fontface="bold") + 
  annotate("text", x = 0.1, y = 0.95, 
           label = "Angry" , color="black", alpha = 0.7, fontface="bold") +
  annotate("text", x = 0.9, y = 0.05, 
           label = "Calm" , color="black",alpha = 0.7, fontface="bold") + 
  annotate("text", x = 0.9, y = 0.95, 
           label = "Happy" , color="black", alpha = 0.7, fontface="bold") +
  geom_hline(yintercept=0.5, linetype="dashed", color = "orange") +
  geom_vline(xintercept = 0.5, linetype="dashed", 
                color = "orange")

ggplotly(scatterplotbillie)

```

*** 

I also want to see how much the influence of Billie Eilish is on the wrapped playlist of 2019 because 20 numbers are from her in this list. As expected most of her songs fall under the category 'sad'. Because she had a significant amount of numbers in the 2019 wrapped playlist, this can also account for the more low energy songs in this playlist relative to 2020 and 2018. 


### The most calm song out of all the different playlists chromogram: **Fica tudo bem**

```{r}
fica <-
  get_tidy_audio_analysis("0trB3R0YBk3vGrGm5YSUTv") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

```

```{r, fig.width=8}
fica %>%
  mutate(pitches = map(pitches, compmus_normalise, "manhattan")) %>%
  compmus_gather_chroma() %>% 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()

```

***
The chromagram of 'Fica Tudo Bem' looks very regular with the time intervals being the same between the different notes played. Maybe this causes the song to sound extra calm. The most used keys are: C, D#/Eb and F. A comparison with the most angry song chromagram is made in the next storyboard. 


### The most angry song out of the playlists chromogram: **Like I do**

```{r}
ido <-
  get_tidy_audio_analysis("6RnkFd8Fqqgk1Uni8RgqCQ") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)
```

```{r, fig.width=8}
ido %>%
  mutate(pitches = map(pitches, compmus_normalise, "manhattan")) %>%
  compmus_gather_chroma() %>% 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()
```

***
The chromogram of 'Like I do' looks a lot less regular than the one of the calm song. The notes have different magnitude ranges at different time intervals. For example the first 50 seconds D#/Eb is played frequently than 50 seconds not anymore and after 100 seconds it comes back again in magnitude. The most used keys are: C, G#/Ab and G. 

### Further discussion of the chroma and timbre features of the outliers in the corpus: **Pastempomat**

```{r, fig.width=6}
polen <-
  get_tidy_audio_analysis("5hYQAKnuCccdHmp0zCjLlE") %>%
  compmus_align(bars, segments) %>%
  select(bars) %>%
  unnest(bars) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )
bind_rows(
  polen %>%
    compmus_self_similarity(pitches, "aitchison") %>%
    mutate(d = d / max(d), type = "Chroma"),
  polen %>%
    compmus_self_similarity(timbre, "euclidean") %>%
    mutate(d = d / max(d), type = "Timbre")
) %>%
  mutate() %>%
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(guide = "none", option = "cividis") +
  theme_classic() +
  labs(x = "seconds", y = "") +
  theme(
    strip.text = element_text(size = 14)
  )

```


*** 

Pastempomat was one of the songs in the 2018 wrapped playlist that I did not listen to before. That is why I want to look at the details of the song to try to understand why spotify has put the song in my wrapped 2018 playlist. 

The chroma- and timbre-based self-similarity matrices show a clear structure in the song. The segments represent the bars of the song. In the chroma-based SSM especially a lot of paths are visible. In the song you can hear this as certain order of cords being repeated after each other. The first 10 seconds sound the same as the 10 seconds that follow thereafter. In the timbre-based SSM this shows a block-like structure for the first 20 seconds because of the homogeneous harmony in the sound. Both SSM show a bright vertical yellow line around 125 seconds. This is the start of the bridge where the background music stops and you just hear the singer sing. Another point where the background music stops is around 145 seconds which is again shown by bright vertical line in the timbre-based SSM but not in the chroma-based SSM. 


### Further discussion of the chroma and timbre features of the outliers in the corpus: **Fica tudo bem**

```{r, fig.width=6}

Fica <-
  get_tidy_audio_analysis("0trB3R0YBk3vGrGm5YSUTv") %>%
  compmus_align(bars, segments) %>%
  select(bars) %>%
  unnest(bars) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )
bind_rows(
  Fica %>%
    compmus_self_similarity(pitches, "aitchison") %>%
    mutate(d = d / max(d), type = "Chroma"),
  Fica %>%
    compmus_self_similarity(timbre, "cosine") %>%
    mutate(d = d / max(d), type = "Timbre")
) %>%
  mutate() %>%
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(guide = "none", option = "cividis") +
  theme_classic() +
  labs(x = "", y = "")+
  theme(
    strip.text = element_text(size = 14)
  )

```

***

Just like the chromagram was very regular in structure for Fica tudo bem, the chroma-based self-similarity matrix is too. It consist of a lot of small block-like structure. The timbre-based SSM has some more differences in structure. Here bigger block-like structure is seen where the harmony of the song is homogeneous. The parts where just instrumental music is played are clearly seen around 50 to 65 seconds and in the last part of the song after 130 seconds. The refrain is being played twice which is seen by the two darker blocks from 35 to 50 seconds and from 95 to 110 seconds. 

### How well can spotify analyse the tonality of the outliers of the corpus?: **Fica tudo bem**

```{r}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

```
```{r}
tudo <-
  get_tidy_audio_analysis("0trB3R0YBk3vGrGm5YSUTv") %>%
  compmus_align(sections, segments) %>%
  select(sections) %>%
  unnest(sections) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )
```

```{r}
tudo %>% 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if desired
    method = "angular",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none", option = "cividis") +
  theme_minimal() +
  labs(x = "Time (s)", y = "") +
  theme(
    plot.title = element_text(size=14, 
    margin = margin(10, 0, 10, 0), hjust = 0.5)
  )+
  ggtitle("Keygram Fica tudo bem")
```

***

On the left you can see another analysis of the most calm song out of the corpus: Fica tudo bem. The song is divided into sections and for each section Spotify estimated the key. The key is estimated by computing the distance of the Spotify chroma vectors to the Krumhansl-Kessler key profiles. 

As one can see right away is that the key estimates are very blurry. A clear key estimate would show one key having a dark blue color block and the others a more brighter for example yellow color. In this key gram a lot of keys have a darker blue color and thus it is not clear how the tonality in the song changes. 

A final observation in the keygram are the vertical yellow parts, first one being seen between 50 and 65 seconds. In the song these sections are the parts where just instrumentals are playing and where the key-finding algorithm has the most trouble finding an estimate for the key of that section. 


### How well can spotify analyse the tonality of the outliers of the corpus?: **Like I do**

```{r}
like <-
  get_tidy_audio_analysis("6RnkFd8Fqqgk1Uni8RgqCQ") %>%
  compmus_align(sections, segments) %>%
  select(sections) %>%
  unnest(sections) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

```

```{r}
like %>% 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if desired
    method = "angular",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none", option = "cividis") +
  theme_minimal() +
  labs(x = "Time (s)", y = "") +
  theme(
    plot.title = element_text(size=14, 
    margin = margin(10, 0, 10, 0), hjust = 0.5)
  )+
  ggtitle("Keygram Like I do")
```

*** 

On the left you can see another analysis of the most angry song out of the corpus: Like I do. Just like for the song: Fica tudo bem, this song is divided into sections and for each section Spotify estimated the key. The key is estimated by computing the distance of the Spotify chroma vectors to the Krumhansl-Kessler key profiles. 

A slighty better estimate of keys is made in this keygram compared to the previous one. It is clear that the song starts out in the key C minor. And around 130 to 140 seconds the key is a Ab major or a F minor. Looking back at the chromagram of Like I do one could also see a bigger magnitude for the Ab pitch around this time in the song compared to the other pitches. 

But for most of the song the key estimates are very blurry and not clear. Especially in the refrain parts of the song at around 45-95s and 145-190s. 

After looking at the two keygrams it can be concluded that Spotify has trouble analysing the tonality of the outliers in my corpus. 